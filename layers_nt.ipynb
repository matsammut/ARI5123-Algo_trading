{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6193a068",
   "metadata": {},
   "source": [
    "In this notebook we have the functions used for the building of the technical and embedding layers respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dffb4d",
   "metadata": {},
   "source": [
    "We made use of the following 7 indicators for the technical layer:\n",
    "1. Stochastic %K\n",
    "2. William’s %R\n",
    "3. Stochastic %D\n",
    "4. A/D Oscillator\n",
    "5. Momentum\n",
    "6. Disparity\n",
    "7. Rate of Change\n",
    "\n",
    "As inspired by Combining News and Technical Indicators in Daily Stock Price Trends Prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfbe38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def calculate_technical_indicator(start_date,end_date):\n",
    "    lookback = 3\n",
    "    start_date_lookback = (datetime.strptime(start_date, \"%Y-%m-%d\") - timedelta(days=lookback))\n",
    "    technical_layer = yf.download(\"^GSPC\", start=start_date_lookback, end=end_date, interval=\"1d\")\n",
    "   \n",
    "    # 1. Stochastic %K\n",
    "    low_min = technical_layer['Low'].rolling(window=lookback).min()\n",
    "    high_max = technical_layer['High'].rolling(window=lookback).max()\n",
    "    technical_layer['Stochastic_%K'] = 100 * ((technical_layer['Close'] - low_min) / (high_max - low_min))\n",
    "\n",
    "    # 2. Williams %R\n",
    "    technical_layer[\"Williams_%R\"] = -100 * ((high_max - technical_layer['Close']) / (high_max - low_min))\n",
    "\n",
    "    # 3. Stochastic %D \n",
    "    technical_layer['Stochastic_%D'] = technical_layer['Stochastic_%K'].rolling(window=lookback-1).mean()\n",
    "\n",
    "    # 4. A/D Oscillator (Accumulation/Distribution Line)\n",
    "    ad = ((technical_layer['Close'] - technical_layer['Low']) - (technical_layer['High'] - technical_layer['Close'])) / (technical_layer['High'] - technical_layer['Low']) * technical_layer['Volume']\n",
    "    technical_layer['AD_Line'] = ad.cumsum()\n",
    "    technical_layer['AD_Oscillator'] = technical_layer['AD_Line'] - technical_layer['AD_Line'].shift(lookback)\n",
    "\n",
    "    # 5. Momentum \n",
    "    technical_layer['Momentum'] = technical_layer['Close'] - technical_layer['Close'].shift(lookback)\n",
    "\n",
    "    # 6. Disparity \n",
    "    technical_layer['Disparity'] = (technical_layer['Close'] / technical_layer['Close'].rolling(window=lookback).mean()) * 100\n",
    "\n",
    "    # 7. Rate of Change (ROC)\n",
    "    technical_layer['ROC'] = ((technical_layer['Close'] - technical_layer['Close'].shift(lookback)) / technical_layer['Close'].shift(lookback)) * 100\n",
    "    technical_layer = technical_layer.loc[start_date:].reset_index()\n",
    "    technical_layer = technical_layer.dropna()\n",
    "    return technical_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb700ee0",
   "metadata": {},
   "source": [
    "Generating the Data for the embedding layer, we preprocess the title by setting it to lower and converting the quotation marks, after this it performs tokenization by braking down a sentence into individual lexeems then assembling them  into word tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe39085",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "\n",
    "def preprocess_title(title):\n",
    "    title = str(title).lower()\n",
    "    title = title.replace(\"’\", \"'\").replace(\"‘\", \"'\").replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "    tokens = re.findall(r\"\\b[a-zA-Z']+\\b\", title)\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaee6d1",
   "metadata": {},
   "source": [
    "Here from the tokens and the word2vec model we are generating A single vector from the previously generated toekns, that represents the average (mean) of the word vectors in the sentence. If none of the words are found in the model, it returns a zero vector of the same size as the word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb920a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_vector(tokens,model):\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e43ba43",
   "metadata": {},
   "source": [
    "Here we processes a CSV file containing news article titles and dates to generate daily news embeddings. It reads the data, and calls the above functions. It then trains a Word2Vec model on the tokenized titles and computes sentence vectors for each article by averaging its word embeddings. \n",
    "\n",
    "These vectors are then aggregated daily by averaging them per date, resulting in a single embedding vector for each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4966a6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_embedding_layer(filename,trading_days):\n",
    "    embedding_layer = pd.read_csv(filename)  \n",
    "    embedding_layer = embedding_layer.sort_values('Date').reset_index(drop=True)  \n",
    "    embedding_layer['Date'] = pd.to_datetime(embedding_layer['Date']).dt.tz_localize(None).dt.date\n",
    "\n",
    "    token_list = embedding_layer['Article_title'].apply(preprocess_title) \n",
    "    model = Word2Vec(sentences=token_list, vector_size=100, window=5, min_count=1, workers=4)\n",
    "    embedding_layer['sentence_vector'] = token_list.apply(lambda tokens: get_sentence_vector(tokens, model))\n",
    "\n",
    "    daily_news = embedding_layer.groupby('Date')['sentence_vector'].apply(lambda vecs: np.mean(list(vecs), axis=0)).reset_index()\n",
    "    daily_news_trading_days = daily_news[daily_news['Date'].isin(trading_days)].reset_index(drop=True)   \n",
    "    return daily_news_trading_days"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
