{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3ae4e62",
   "metadata": {},
   "source": [
    "The goal of this code is the building of an SI-RCNN model to forcast intraday directional movements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe87951a",
   "metadata": {},
   "source": [
    "The first step is the loading of seven technical indicators from our stock of choice. For the remit of this assignment we used the S&P 500.\n",
    "\n",
    "We made use of the following 7 indicators:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bc86e1",
   "metadata": {},
   "source": [
    "1. Stochastic %K\n",
    "2. William’s %R\n",
    "3. Stochastic %D\n",
    "4. A/D Oscillator\n",
    "5. Momentum\n",
    "6. Disparity\n",
    "7. Rate of Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d1dfe32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import yfinance as yf\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import ta\n",
    "\n",
    "# data = yf.download(\"^GSPC\", start=\"2023-01-01\", end=\"2025-01-01\", interval=\"1d\")\n",
    "# data.dropna(inplace=True)\n",
    "\n",
    "# # 1. SMA (Simple Moving Average - 20 days)\n",
    "# data['SMA_20'] = data['Close'].rolling(window=20).mean()\n",
    "\n",
    "# # 2. EMA (Exponential Moving Average - 20 days)\n",
    "# data['EMA_20'] = data['Close'].ewm(span=20, adjust=False).mean()\n",
    "\n",
    "# # 3. RSI (Relative Strength Index - 14 days)\n",
    "# delta = data['Close'].diff()\n",
    "# gain = np.where(delta > 0, delta, 0)\n",
    "# loss = np.where(delta < 0, -delta, 0)\n",
    "# avg_gain = pd.Series(gain.reshape(-1)).rolling(window=14).mean()\n",
    "# avg_loss = pd.Series(loss.reshape(-1)).rolling(window=14).mean()\n",
    "# rs = avg_gain / avg_loss\n",
    "# data['RSI_14'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "# # 4. MACD (Moving Average Convergence Divergence)\n",
    "# ema_12 = data['Close'].ewm(span=12, adjust=False).mean()\n",
    "# ema_26 = data['Close'].ewm(span=26, adjust=False).mean()\n",
    "# data['MACD'] = ema_12 - ema_26\n",
    "\n",
    "# # 5. Stochastic Oscillator %K (14-day)\n",
    "# low_14 = data['Low'].rolling(window=14).min()\n",
    "# high_14 = data['High'].rolling(window=14).max()\n",
    "# data['Stochastic_K'] = 100 * ((data['Close'] - low_14) / (high_14 - low_14))\n",
    "\n",
    "# # 6. ATR (Average True Range - 14 days)\n",
    "# high_low = data['High'] - data['Low']\n",
    "# high_close = np.abs(data['High'] - data['Close'].shift())\n",
    "# low_close = np.abs(data['Low'] - data['Close'].shift())\n",
    "# true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "# data['ATR'] = true_range.rolling(window=14).mean()\n",
    "\n",
    "# # 7. OBV (On-Balance Volume)\n",
    "# obv = (np.sign(data['Close'].diff()) * data['Volume']).fillna(0).cumsum()\n",
    "# data['OBV'] = obv\n",
    "\n",
    "# # Keep only indicator columns\n",
    "# indicators = data[['SMA_20', 'EMA_20', 'RSI_14', 'MACD', 'Stochastic_K', 'ATR', 'OBV']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "686f9a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price        Date Stochastic_%K Williams_%R Stochastic_%D AD_Oscillator  \\\n",
      "Ticker                                                                    \n",
      "15     2008-01-24     47.148721  -52.851279     34.063842 -1.397934e+09   \n",
      "16     2008-01-25     37.795634  -62.204366     40.550381 -1.486316e+09   \n",
      "17     2008-01-28     52.368422  -47.631578     45.770926  1.570116e+09   \n",
      "18     2008-01-29     58.004306  -41.995694     49.389454  8.574271e+09   \n",
      "19     2008-01-30     53.923576  -46.076424     54.765434 -6.741248e+08   \n",
      "\n",
      "Price    Momentum  Disparity       ROC  \n",
      "Ticker                                  \n",
      "15     -95.090088  98.187044 -6.570807  \n",
      "16     -81.020020  97.036433 -5.739466  \n",
      "17     -62.220093  99.060320 -4.393516  \n",
      "18     -27.889893  99.815992 -2.006193  \n",
      "19     -53.319946  99.618459 -3.783891  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\gianm\\AppData\\Local\\Temp\\ipykernel_92500\\3710852846.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  technical_indicators.dropna(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "\n",
    "# Ensure the data is sorted by date\n",
    "technical_layer = yf.download(\"^GSPC\", start=\"2008-01-01\", end=\"2013-01-01\", interval=\"1d\")\n",
    "technical_layer.reset_index(inplace=True)\n",
    "technical_layer.dropna(inplace=True)\n",
    "\n",
    "\n",
    "# Parameters\n",
    "lookback = 14  # typical lookback for most of these indicators\n",
    "\n",
    "# 1. Stochastic %K\n",
    "low_min = technical_layer['Low'].rolling(window=lookback).min()\n",
    "high_max = technical_layer['High'].rolling(window=lookback).max()\n",
    "technical_layer['Stochastic_%K'] = 100 * ((technical_layer['Close'] - low_min) / (high_max - low_min))\n",
    "\n",
    "# 2. Williams %R\n",
    "technical_layer[\"Williams_%R\"] = -100 * ((high_max - technical_layer['Close']) / (high_max - low_min))\n",
    "\n",
    "# 3. Stochastic %D (3-period SMA of %K)\n",
    "technical_layer['Stochastic_%D'] = technical_layer['Stochastic_%K'].rolling(window=3).mean()\n",
    "\n",
    "# 4. A/D Oscillator (Accumulation/Distribution Line)\n",
    "ad = ((technical_layer['Close'] - technical_layer['Low']) - (technical_layer['High'] - technical_layer['Close'])) / (technical_layer['High'] - technical_layer['Low']) * technical_layer['Volume']\n",
    "technical_layer['AD_Line'] = ad.cumsum()\n",
    "technical_layer['AD_Oscillator'] = technical_layer['AD_Line'] - technical_layer['AD_Line'].shift(lookback)\n",
    "\n",
    "# 5. Momentum (Close - Close n periods ago)\n",
    "technical_layer['Momentum'] = technical_layer['Close'] - technical_layer['Close'].shift(lookback)\n",
    "\n",
    "# 6. Disparity (Close / Moving Average * 100)\n",
    "technical_layer['Disparity'] = (technical_layer['Close'] / technical_layer['Close'].rolling(window=lookback).mean()) * 100\n",
    "\n",
    "# 7. Rate of Change (ROC)\n",
    "technical_layer['ROC'] = ((technical_layer['Close'] - technical_layer['Close'].shift(lookback)) / technical_layer['Close'].shift(lookback)) * 100\n",
    "\n",
    "# Display relevant columns\n",
    "technical_indicators = technical_layer[['Date', 'Stochastic_%K', 'Williams_%R', 'Stochastic_%D','AD_Oscillator', 'Momentum', 'Disparity', 'ROC']]\n",
    "technical_indicators.dropna(inplace=True)\n",
    "print(technical_indicators.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add7467f",
   "metadata": {},
   "source": [
    "The next step was to create another input of financial news sentence embeddings, for this we used the FNSPID dataset which hold millions of financial news records covering S&P 500 companies.\n",
    "\n",
    "https://github.com/Zdong104/FNSPID_Financial_News_Dataset\n",
    "\n",
    "We manually cleaned the dataset by removing some of the lower rows that had garbage data upon downloaded. Following this we loaded it into python sorted it by date and removed all other columns before saving it again so we may reduce how many times this section is run. We then reduce it to the 5 year date range of 2008-2013 which will be our training window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c317118e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_csv = pd.read_csv('All_external.csv',usecols=['Article_title', 'Date'])  \n",
    "# full_csv = full_csv.sort_values('Date').reset_index(drop=True)  \n",
    "\n",
    "# full_csv = full_csv.set_index('Date')\n",
    "# full_csv.to_csv('Sorted_Articles.csv')    \n",
    "\n",
    "# filtered_layer = embedding_layer.loc['2008-01-01':'2013-12-31']\n",
    "# filtered_layer.to_csv('Sorted_Articles_Reduced.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd364db",
   "metadata": {},
   "source": [
    "First we tokenize the titles, we handled quotations as this caused some parsing issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cad0751f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "\n",
    "embedding_layer = pd.read_csv('Sorted_Articles_Reduced.csv')  \n",
    "embedding_layer = embedding_layer.sort_values('Date').reset_index(drop=True)  \n",
    "\n",
    "def preprocess_title(title):\n",
    "    title = str(title).lower()\n",
    "    title = title.replace(\"’\", \"'\").replace(\"‘\", \"'\").replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "    tokens = re.findall(r\"\\b[a-zA-Z']+\\b\", title)\n",
    "    return tokens\n",
    "\n",
    "token_list = embedding_layer['Article_title'].apply(preprocess_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bff039",
   "metadata": {},
   "source": [
    "Next we create a sentence embeddings by averaging the word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7059066b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=token_list, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "def get_sentence_vector(tokens):\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "embedding_layer['sentence_vector'] = token_list.apply(get_sentence_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188287e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 1 prediction: [0, 1] -> down\n",
      "Day 2 prediction: [0, 1] -> down\n",
      "Day 3 prediction: [0, 1] -> down\n",
      "Day 4 prediction: [0, 1] -> down\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the combined CNN + LSTM model\n",
    "class NewsCNN_LSTM(nn.Module):\n",
    "    def __init__(self, embedding_dim=100, cnn_out_channels=64, lstm_hidden_size=128, lstm_layers=1, dropout=0.5, num_classes=2):\n",
    "        super(NewsCNN_LSTM, self).__init__()\n",
    "\n",
    "        # CNN Layer\n",
    "        self.conv1 = nn.Conv1d(in_channels=embedding_dim, out_channels=cnn_out_channels, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)  # Optional: downsample after CNN\n",
    "\n",
    "        # LSTM Layer\n",
    "        self.lstm = nn.LSTM(input_size=cnn_out_channels, hidden_size=lstm_hidden_size,\n",
    "                            num_layers=lstm_layers, batch_first=True, bidirectional=False)\n",
    "\n",
    "        # Fully Connected + Softmax\n",
    "        self.fc = nn.Linear(lstm_hidden_size, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, embedding_dim)\n",
    "        x = x.permute(0, 2, 1)  # (batch_size, embedding_dim, seq_len) for Conv1D\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = x.permute(0, 2, 1)  # Back to (batch_size, seq_len, channels) for LSTM\n",
    "\n",
    "        # LSTM layer\n",
    "        lstm_out, _ = self.lstm(x)  # lstm_out: (batch_size, seq_len, lstm_hidden_size)\n",
    "\n",
    "        # Take last timestep output\n",
    "        last_timestep = lstm_out[:, -1, :]  # (batch_size, lstm_hidden_size)\n",
    "\n",
    "        logits = self.fc(last_timestep)\n",
    "        probs = self.softmax(logits)\n",
    "\n",
    "        return probs\n",
    "\n",
    "# Example model instantiation\n",
    "model = NewsCNN_LSTM()\n",
    "\n",
    "# Example dummy input: batch of 4 days, each with max_seq_len=10, embedding_dim=100\n",
    "batch_size = 4\n",
    "max_seq_len = 10\n",
    "embedding_dim = 100\n",
    "\n",
    "dummy_input = torch.randn(batch_size, max_seq_len, embedding_dim)\n",
    "\n",
    "# Forward pass\n",
    "output = model(dummy_input)\n",
    "\n",
    "# Example interpretation\n",
    "for i, prediction in enumerate(output):\n",
    "    label = [1,0] if prediction.argmax().item() == 0 else [0,1]\n",
    "    direction = 'up' if label == [1,0] else 'down'\n",
    "    print(f\"Day {i+1} prediction: {label} -> {direction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bb2422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date\n",
      "2020-03-31 00:00:00 UTC    [[0.00038041366, 0.00038732446, 0.00061241444,...\n",
      "2020-04-01 00:00:00 UTC    [[-0.0010377998, 0.00042375515, -0.0011617246,...\n",
      "2020-04-02 00:00:00 UTC    [[-0.0023824708, -0.0012906671, 0.0009930803, ...\n",
      "2020-04-06 00:00:00 UTC    [[-0.0007787028, 0.0014723241, -0.0006502996, ...\n",
      "2020-04-08 00:00:00 UTC    [[-0.0023002836, 0.001976771, -0.00018580035, ...\n",
      "2020-04-14 00:00:00 UTC    [[-0.0010352622, -0.0015254191, -0.002226289, ...\n",
      "2020-04-22 00:00:00 UTC    [[-0.00014356375, -0.00014395872, -0.001554029...\n",
      "2020-04-23 00:00:00 UTC    [[0.0007607373, 0.0022731104, -0.0003411403, 0...\n",
      "2020-04-28 00:00:00 UTC    [[-0.0008626608, -0.0023146372, 0.002436483, 0...\n",
      "2020-05-01 00:00:00 UTC    [[-0.0008346233, 0.0016315253, 0.0006025097, -...\n",
      "2020-05-05 00:00:00 UTC    [[-0.00014000069, 0.002407807, 0.0018011844, -...\n",
      "2020-05-08 00:00:00 UTC    [[-0.0008903536, 0.0041253697, -0.0019558165, ...\n",
      "2020-05-15 00:00:00 UTC    [[-0.00420256, 0.001401495, 0.0012031626, 0.00...\n",
      "2020-05-16 00:00:00 UTC    [[0.0015997442, 0.0040907594, 0.0014146743, -0...\n",
      "2020-05-18 00:00:00 UTC    [[-0.0014908734, 0.0027777825, 0.001969549, 0....\n",
      "2020-05-21 00:00:00 UTC    [[0.0014033166, 0.0020460426, -0.0011179068, -...\n",
      "2020-05-22 00:00:00 UTC    [[-0.0023984618, 0.00049329695, 0.0017930693, ...\n",
      "2020-05-22 04:06:17 UTC    [[0.0017553534, 0.0009279386, -0.0027275821, -...\n",
      "2020-05-22 04:37:59 UTC    [[-0.0023270848, -0.0008484444, 0.0018316925, ...\n",
      "2020-05-22 05:07:04 UTC    [[0.001953044, 0.00066828105, 0.0009675104, 0....\n",
      "2020-05-22 05:36:20 UTC    [[-0.0015265128, -0.0010131574, 0.0028580765, ...\n",
      "2020-05-22 07:23:25 UTC    [[-0.0019872696, -0.0021185125, 0.0005397282, ...\n",
      "2020-05-22 07:38:59 UTC    [[-0.0028656696, 0.00084226695, 0.0011121533, ...\n",
      "2020-05-22 08:45:06 UTC    [[-0.0009733803, -0.0018607078, 0.0005945036, ...\n",
      "2020-05-26 00:30:07 UTC    [[-0.0041690194, 0.004640135, -0.0011659208, -...\n",
      "2020-06-03 06:45:20 UTC    [[0.0019277496, -0.0028309973, -0.0026490756, ...\n",
      "2020-06-05 06:30:54 UTC    [[0.0027100462, -0.00020415589, -0.002690406, ...\n",
      "Name: sentence_vector, dtype: object\n",
      "tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0812, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0804, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.1052, 0.0809, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0845, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.1052, 0.0836, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(day_to_vectors)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
